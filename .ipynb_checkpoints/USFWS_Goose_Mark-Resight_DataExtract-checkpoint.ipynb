{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USFWS_Goose_Mark-Resight_DataExtract.py\n",
    "### Version: 6/2/2022\n",
    "### Author: Khem So, khem_so@fws.gov, (503) 231-6839\n",
    "### Abstract: This Python 3 script pulls data from the USFWS_Goose_Mark-Resight ArcGIS Online feature service and performs joins and merges to result in combined datasets matching the Migratory Birds program template for dusky Canada goose mark-resight data and refuge/location-specific extracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import pandas as pd\n",
    "from arcgis import GIS\n",
    "import time, os, fnmatch, shutil\n",
    "import janitor # https://pyjanitor-devs.github.io/pyjanitor/\n",
    "import openpyxl # https://openpyxl.readthedocs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ArcGIS Online stores date-time information in UTC by default. This function uses the pytz package to convert time zones and can be used to convert from UTC (\"UTC\") to localized time. For example, localized \"US/Pacific\" is either Pacific Standard Time UTC-8 or Pacific Daylight Time UTC-7 depending upon time of year.\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "def change_timezone_of_field(df, source_date_time_field, new_date_time_field_suffix, source_timezone, new_timezone):\n",
    "    \"\"\"Returns the values in *source_date_time_field* with its timezone converted to a new timezone within a new field *new_date_time_field*\n",
    "    : param df: The name of the spatially enabled or pandas DataFrame containing datetime fields\n",
    "    : param source_date_time_field: The name of the datetime field whose timezone is to be changed\n",
    "    : param new_date_time_field_suffix: Suffix appended to the end of the name of the source datetime field. This is used to create the new date time field name.\n",
    "    : param source_timezone: The name of the source timezone\n",
    "    : param new_timezone: The name of the converted timezone. For possible values, see https://gist.github.com/heyalexej/8bf688fd67d7199be4a1682b3eec7568\n",
    "    \"\"\"\n",
    "    # Define the source timezone in the source_date_time_field\n",
    "    df[source_date_time_field] = df[source_date_time_field].dt.tz_localize(source_timezone)\n",
    "    # Define the name of the new date time field\n",
    "    new_date_time_field = source_date_time_field + new_date_time_field_suffix\n",
    "    # Convert the datetime in the source_date_time_field to the new timezone in a new field called new_date_time_field\n",
    "    df[new_date_time_field] = df[source_date_time_field].dt.tz_convert(new_timezone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function converts Python datetime64 fields to %m/%d/%Y %H:%M:%S %Z%z format\n",
    "def archive_dt_field(df):\n",
    "    \"\"\"Selects fields with data types of 'datetime64[ns, UTC]','datetime64[ns, US/Pacific]' and converts to %m/%d/%Y %H:%M:%S %Z%z format for archiving to Excel\n",
    "    : param df: The name of the spatially enabled or pandas DataFrame containing datetime fields\n",
    "    \"\"\"\n",
    "    archive_dt_field_list = df.select_dtypes(include=['datetime64[ns, UTC]','datetime64[ns, US/Pacific]'])\n",
    "    for col in archive_dt_field_list:\n",
    "        df[col] = df[col].dt.strftime('%m/%d/%Y %H:%M:%S %Z%z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function converts strings in a dataframe to uppercase\n",
    "def upper_consistent(df):\n",
    "    df = df.apply(lambda x: x.str.upper() if x.dtype == \"object\" else x) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Allow authentication via login to U.S. Fish & Wildlife Service ArcGIS Online account via ArcGIS Pro\n",
    "gis = GIS(\"pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Enter start and end dates of interest\n",
    "# uncomment next line to use ArcGIS interface, otherwise hard coding year\n",
    "# startdate = arcpy.GetParameterAsText(0)\n",
    "# enddate = arcpy.GetParameterAsText(1)\n",
    "start_date = \"09-01-2021\"\n",
    "end_date = \"06-01-2022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Enter path for local file saving\n",
    "# uncomment next line to use ArcGIS interface, otherwise hard coding out_workspace\n",
    "# out_workspace = arcpy.GetParameterAsText(1)\n",
    "out_workspace = \"C:/Users/kso/Desktop/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create timestamp for file naming\n",
    "t = time.localtime()\n",
    "timestamp = time.strftime('%Y-%m-%d_%H%M', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Paths to ArcGIS Online data\n",
    "# To populate Service ItemId, go to Feature Service webpage and in bottom right corner, click on the View link.\n",
    "# Current Feature Service webpage: https://fws.maps.arcgis.com/home/item.html?id=87e3dfd8e8974fac84d29b7092025a0d\n",
    "ServiceItemID = gis.content.get(\"87e3dfd8e8974fac84d29b7092025a0d\")\n",
    "\n",
    "### There are separate methods for pulling spatial versus non-spatial data into Python. Spatial layers will become Spatially Enabled DataFrame objects. Non-spatial data will become regular pandas DataFrame objects.\n",
    "## Define variables pointing to spatial layers\n",
    "MetadataLyr = ServiceItemID.layers[0]\n",
    "ObservationPointLyr = ServiceItemID.layers[1]\n",
    "## Create Spatially Enabled DataFrame objects\n",
    "sedfMetadata = pd.DataFrame.spatial.from_layer(MetadataLyr)\n",
    "sedfObservationPoint = pd.DataFrame.spatial.from_layer(ObservationPointLyr)\n",
    "\n",
    "## Define variables point to non-spatial (tabular) data\n",
    "OtherSpeciesBands = r\"https://services.arcgis.com/QVENGdaPbd4LUkLV/ArcGIS/rest/services/service_e01fb68477c047f4ab25ad3e6c30ac1b/FeatureServer/2\"\n",
    "\n",
    "## Convert AGOL table to NumPy Array and then to pandas DataFrames\n",
    "naOtherSpeciesBands = arcpy.da.TableToNumPyArray(OtherSpeciesBands,[\"objectid\",\"globalid\",\"SpeciesText\",\"BandNote\",\"OtherSpBandCode\",\"OtherBandJoin\",\"parentglobalid\",\"CreationDate\",\"Creator\",\"EditDate\",\"Editor\"])\n",
    "dfOtherSpeciesBands = pd.DataFrame(naOtherSpeciesBands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter sedfMetadata by date range\n",
    "sedfMetadataYYYY = sedfMetadata.filter_date(\"EffortDate\", start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use change_timezone_of_field function to convert all datetime fields in dataframe from UTC to Pacific within new field with _Pacific suffix\n",
    "for col in sedfMetadata.columns:\n",
    "     if sedfMetadata[col].dtype == 'datetime64[ns]':\n",
    "         change_timezone_of_field(sedfMetadata, col, \"_Pacific\", \"UTC\", \"US/Pacific\")\n",
    "\n",
    "for col in sedfObservationPoint.columns:\n",
    "     if sedfObservationPoint[col].dtype == 'datetime64[ns]':\n",
    "         change_timezone_of_field(sedfObservationPoint, col, \"_Pacific\", \"UTC\", \"US/Pacific\")\n",
    "\n",
    "for col in dfOtherSpeciesBands.columns:\n",
    "     if dfOtherSpeciesBands[col].dtype == 'datetime64[ns]':\n",
    "         change_timezone_of_field(dfOtherSpeciesBands, col, \"_Pacific\", \"UTC\", \"US/Pacific\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export raw data frames as backup\n",
    "## Use archive_dt_field function to convert Python date time into format Excel can read more easily\n",
    "archive_dt_field(sedfMetadata)\n",
    "archive_dt_field(sedfObservationPoint)\n",
    "archive_dt_field(dfOtherSpeciesBands)\n",
    "\n",
    "## Create export paths for backup and writes to Excel spreadsheet\n",
    "writer = pd.ExcelWriter(os.path.join(out_workspace,('USFWS_Goose_Mark-Resight_BKUP_' + timestamp + '.xlsx')))\n",
    "sedfMetadata.to_excel(writer, 'Metadata')\n",
    "sedfObservationPoint.to_excel(writer, 'ObservationPoint')\n",
    "dfOtherSpeciesBands.to_excel(writer, 'OtherSpeciesBands')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Populate ObserverText column\n",
    "sedfMetadataYYYY[\"ObserverText\"] = sedfMetadataYYYY[\"Observer\"]\n",
    "sedfMetadataYYYY.loc[sedfMetadataYYYY[\"Observer\"] == \"Other\", \"ObserverText\"] = sedfMetadataYYYY[\"ObserverOther\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Populate State column based on Refuge\n",
    "WA_filter = ['Willapa NWR', 'Julia Butler Hansen Refuge for the Columbian White-tailed Deer', 'Ridgefield NWR']\n",
    "OR_filter = ['Lewis and Clark NWR', 'Tualatin River NWR', 'Wapato Lake NWR', 'Baskett Slough NWR', 'Ankeny NWR', 'William L. Finley NWR', 'Nestucca Bay NWR']\n",
    "sedfMetadataYYYY.loc[sedfMetadataYYYY[\"SiteName\"].isin(WA_filter), \"State\"] = \"WA\"\n",
    "sedfMetadataYYYY.loc[sedfMetadataYYYY[\"SiteName\"].isin(OR_filter), \"State\"] = \"OR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Join sedfMetadataYYYY with sedfObservationPoint\n",
    "sedfMetadataYYYY_ObservationPoint = pd.merge(sedfMetadataYYYY,sedfObservationPoint, how=\"inner\", left_on=\"globalid\", right_on=\"parentglobalid\")\n",
    "sedfMetadataYYYY_ObservationPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Populate LatitudeDD and LongitudeDD fields based on whether there are prepopulated values\n",
    "Prepopulated_filter = ['Ridgefield NWR', 'Tualatin River NWR', 'Wapato Lake NWR']\n",
    "sedfMetadataYYYY_ObservationPoint.loc[sedfMetadataYYYY_ObservationPoint[\"SiteName\"].isin(Prepopulated_filter) & sedfMetadataYYYY_ObservationPoint[\"Latitude_Prepopulated\"].notnull(), \"LatitudeDD\"] = sedfMetadataYYYY_ObservationPoint[\"Latitude_Prepopulated\"]\n",
    "sedfMetadataYYYY_ObservationPoint.loc[sedfMetadataYYYY_ObservationPoint[\"SiteName\"].isin(Prepopulated_filter) & sedfMetadataYYYY_ObservationPoint[\"Longitude_Prepopulated\"].notnull(), \"LongitudeDD\"] = sedfMetadataYYYY_ObservationPoint[\"Longitude_Prepopulated\"]\n",
    "sedfMetadataYYYY_ObservationPoint.loc[sedfMetadataYYYY_ObservationPoint[\"SiteName\"].isin(Prepopulated_filter) & sedfMetadataYYYY_ObservationPoint[\"Latitude_Prepopulated\"].isnull(), \"LatitudeDD\"] = sedfMetadataYYYY_ObservationPoint[\"Latitude\"]\n",
    "sedfMetadataYYYY_ObservationPoint.loc[sedfMetadataYYYY_ObservationPoint[\"SiteName\"].isin(Prepopulated_filter) & sedfMetadataYYYY_ObservationPoint[\"Longitude_Prepopulated\"].isnull(), \"LongitudeDD\"] = sedfMetadataYYYY_ObservationPoint[\"Longitude\"]\n",
    "sedfMetadataYYYY_ObservationPoint.loc[~sedfMetadataYYYY_ObservationPoint[\"SiteName\"].isin(Prepopulated_filter), \"LatitudeDD\"] = sedfMetadataYYYY_ObservationPoint[\"Latitude\"]\n",
    "sedfMetadataYYYY_ObservationPoint.loc[~sedfMetadataYYYY_ObservationPoint[\"SiteName\"].isin(Prepopulated_filter), \"LongitudeDD\"] = sedfMetadataYYYY_ObservationPoint[\"Longitude\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate column for sum total of count data\n",
    "col_list= ['DuskyCount', 'WuskyCount', 'WesternCount', 'TavLessCount', 'TavCount', 'LessCount', 'AleutianCount', 'CacklingCount', 'UnknownCanadaCount', 'UnknownCacklerCount', 'WhiteFrontedCount', 'SnowCount', 'RossCount', 'UnknownGooseCount', 'TrumpeterCount', 'TundraCount', 'UnknownSwanCount']\n",
    "sedfMetadataYYYY_ObservationPoint[\"Total\"] = sedfMetadataYYYY_ObservationPoint[col_list].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create data frame for dusky neckband/collar data\n",
    "dfDuskyNeckband = sedfMetadataYYYY_ObservationPoint.copy()\n",
    "### Reorder columns\n",
    "dfDuskyNeckband = dfDuskyNeckband[['objectid_x', 'globalid_x', 'SiteName', 'SiteNameOther', 'Observer', 'ObserverOther', 'ObserverText', 'State', 'EffortDate', 'EffortDate_Text', 'FormName', 'survey_uuid', 'FormVersion', 'CreationDate_x', 'Creator_x', 'EditDate_x', 'Editor_x', 'SHAPE_x', 'AleutianCount', 'CacklingCount', 'CreationDate_y', 'Creator_y', 'DuskyBandNote', 'DuskyCollar1', 'DuskyCollar2', 'DuskyCollar3', 'DuskyCollar4', 'DuskyCollar5', 'DuskyCollar6', 'DuskyCollar7', 'DuskyCollar8', 'DuskyCollar9', 'DuskyCollar10', 'DuskyCollar11', 'DuskyCollar12', 'DuskyCollar13', 'DuskyCollar14', 'DuskyCollar15', 'DuskyCollar16', 'DuskyCollar17', 'DuskyCollar18', 'DuskyCollar19', 'DuskyCollar20', 'DuskyCollar21', 'DuskyCollar22', 'DuskyCollar23', 'DuskyCollar24', 'DuskyCollar25', 'DuskyCollar26', 'DuskyCollar27', 'DuskyCollar28', 'DuskyCollar29', 'DuskyCollar30', 'DuskyCount', 'EditDate_y', 'Editor_y', 'EffortNotes', 'EffortTime', 'Latitude', 'Latitude_Prepopulated', 'LessCount', 'LocationDescription', 'LocationDescription_ReadOnly', 'LocationOther', 'Longitude', 'Longitude_Prepopulated', 'NumberDuskyCollars', 'RossCount', 'SHAPE_y', 'SnowCount', 'TavCount', 'TavLessCount', 'TotalGeese', 'TrumpeterCount', 'TundraCount', 'UnknownCacklerCount', 'UnknownCanadaCount', 'UnknownGooseCount', 'UnknownSwanCount', 'WesternCount', 'WhiteFrontedCount', 'WuskyCount', 'globalid_y', 'objectid_y', 'parentglobalid', 'CreationDate_Pacific', 'EditDate_Pacific', 'LatitudeDD', 'LongitudeDD', 'Total']]\n",
    "### Drop columns with null values\n",
    "dfDuskyNeckband.dropna(how='all', axis=1, inplace=True)\n",
    "### Calculate field for coordinate precision\n",
    "dfDuskyNeckband[\"Prec\"] = '0'\n",
    "#dfDuskyNeckband[\"Coord1\"] = dfDuskyNeckband[\"LatitudeDD\"].replace(\".\", \"?\") \n",
    "#dfDuskyNeckband[\"Format\"] = \"DD\"\n",
    "dfDuskyNeckband[\"Location\"] = dfDuskyNeckband.SiteName.str.cat(dfDuskyNeckband.LocationDescription,sep=\" \", na_rep = \"\")\n",
    "dfDuskyNeckband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that Dusky collars are uppercase\n",
    "dfDuskyColumns = dfDuskyNeckband.filter(regex='Dusky*',axis=1)\n",
    "dfDuskyColumnsList = list(dfDuskyColumns.columns.values)\n",
    "\n",
    "dfDuskyNeckband[dfDuskyColumnsList] = upper_consistent(dfDuskyNeckband[dfDuskyColumnsList])\n",
    "\n",
    "# Select only rows where DuskyCollar is not null\n",
    "dfDuskyNeckband = dfDuskyNeckband[~dfDuskyNeckband.filter(like='DuskyCollar').isna().all(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame for export to Refuge staff. Use Darwin Core/biotic observation minimum standards.\n",
    "dfDuskyNeckbandNWR = (\n",
    "    pd.DataFrame(dfDuskyNeckband)\n",
    "    .select_columns(['globalid_y', 'SiteName', 'LocationDescription', 'EffortDate_Text', 'EffortTime', 'ObserverText',  'Dusky*', 'Total', 'LatitudeDD', 'LongitudeDD'])\n",
    "    .rename_columns(new_column_names={\"globalid_y\": \"occurrenceID\", \"SiteName\": \"location\", \"EffortTime\": \"eventTime\", \"EffortDate_Text\": \"eventDate\", \"LatitudeDD\": \"decimalLatitude\", \"LongitudeDD\": \"decimalLongitude\", \"ObserverText\": \"recordedBy\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame for export to Migratory Birds. Use Migratory Birds schema.\n",
    "dfDuskyNeckbandMB = (\n",
    "    pd.DataFrame(dfDuskyNeckband)\n",
    "    .select_columns(['globalid_y', 'State', 'EffortDate_Text', 'EffortTime', 'Prec', 'LatitudeDD', 'LongitudeDD', 'ObserverText', 'SiteName', 'LocationDescription', 'Location', 'Dusky*', 'Total'])\n",
    "    .rename_columns(new_column_names={\"globalid_y\": \"globalid\", \"EffortDate_Text\": \"Date\", \"LatitudeDD\": \"Latdd\", \"LongitudeDD\": \"Longdd\", \"ObserverText\": \"Obs\", \"DuskyCount\": \"Pres\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Pivot flock data from wide to long so that each row represents a separate species count, per occurrence\n",
    "dfFlockPivotTemp = (\n",
    "    pd.DataFrame(sedfObservationPoint)\n",
    "    .select_columns(['globalid', 'DuskyCount', 'WuskyCount', 'WesternCount', 'TavLessCount', 'TavCount', 'LessCount', 'AleutianCount', 'CacklingCount', 'UnknownCanadaCount', 'UnknownCacklerCount', 'WhiteFrontedCount', 'SnowCount', 'RossCount', 'UnknownGooseCount', 'TrumpeterCount', 'TundraCount', 'UnknownSwanCount'])\n",
    "    .pivot_longer(\n",
    "    index = 'globalid',\n",
    "    names_to = ('shortName', 'dimension'),\n",
    "    names_sep = 'Count',\n",
    "    sort_by_appearance = True,\n",
    "    )\n",
    "    .remove_columns(['dimension'])\n",
    "    .filter_on(\"value != 0\")\n",
    "    .dropna()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use dictionaries and pandas map function to create new fields for scientific name, FWS taxon code, and ITIS code, based on the short species/subspecies name \n",
    "sciname_dict = {\n",
    "    'Aleutian': 'Branta hutchinsii leucopareia',\n",
    "    'Cackling': 'Branta hutchinsii minima',\n",
    "    'Dusky': 'Branta canadensis occidentalis',\n",
    "    'Less': 'Branta canadensis parvipes',\n",
    "    'Tav': 'Branta hutchinsii taverneri',\n",
    "    'TavLess': 'Branta hutchinsii taverneri x Branta canadensis parvipes',\n",
    "    'Western': 'Branta canadensis moffitti',\n",
    "    'Wusky': 'Branta canadensis moffitti x Branta canadensis occidentalis',\n",
    "    'UnknownCackler': 'Branta hutchinsii',\n",
    "    'UnknownCanada': 'Branta canadensis',\n",
    "    'Ross': 'Chen rossii',\n",
    "    'Snow': 'Chen caerulescens',\n",
    "    'WhiteFronted': 'Anser albifrons',\n",
    "    'UnknownGoose': 'Branta',\n",
    "    'Trumpeter': 'Cygnus buccinator',\n",
    "    'UnknownSwan': 'Cygnus columbianus',\n",
    "    'Tundra': 'Cygnus'\n",
    "}\n",
    "\n",
    "FWSTaxonCode_dict = {\n",
    "    'Aleutian': '604316',\n",
    "    'Cackling': '604318',\n",
    "    'Dusky': '77707',\n",
    "    'Less': '76633',\n",
    "    'Tav': '604320',\n",
    "    'TavLess': '',\n",
    "    'Western': '76632',\n",
    "    'Wusky': '',\n",
    "    'UnknownCackler': '604603',\n",
    "    'UnknownCanada': '76625',\n",
    "    'Ross': '77746',\n",
    "    'Snow': '77742',\n",
    "    'WhiteFronted': '77723',\n",
    "    'UnknownGoose': '76624',\n",
    "    'Trumpeter': '76618',\n",
    "    'UnknownSwan': '76612',\n",
    "    'Tundra': '76609'\n",
    "}\n",
    "\n",
    "ITIS_dict = {\n",
    "    'Aleutian': '714726',\n",
    "    'Cackling': '714727',\n",
    "    'Dusky': '175006',\n",
    "    'Less': '175004',\n",
    "    'Tav': '714728',\n",
    "    'TavLess': '',\n",
    "    'Western': '175003',\n",
    "    'Wusky': '',\n",
    "    'UnknownCackler': '714068',\n",
    "    'UnknownCanada': '174999',\n",
    "    'Ross': '175041',\n",
    "    'Snow': '175038',\n",
    "    'WhiteFronted': '175020',\n",
    "    'UnknownGoose': '174998',\n",
    "    'Trumpeter': '174992',\n",
    "    'UnknownSwan': '174987',\n",
    "    'Tundra': '174984'\n",
    "}\n",
    "\n",
    "dfFlockPivot = dfFlockPivotTemp.copy()\n",
    "dfFlockPivot['scientificName'] = dfFlockPivot['shortName'].map(sciname_dict)\n",
    "dfFlockPivot['fwsTaxonCode'] = dfFlockPivot['shortName'].map(FWSTaxonCode_dict)\n",
    "dfFlockPivot['ITISTaxonCode'] = dfFlockPivot['shortName'].map(ITIS_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create data frame of survey metadata for merging with pivotted long flock data\n",
    "dfCleanMetadata = (\n",
    "    pd.DataFrame(sedfMetadataYYYY_ObservationPoint)\n",
    "    .select_columns(['globalid_x', 'globalid_y', 'SiteName', 'ObserverText', 'EffortDate_Text', 'EffortTime', 'LocationDescription', 'LocationOther', 'EffortNotes', 'LatitudeDD', 'LongitudeDD'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge survey metadata with pivotted long flock data\n",
    "dfFlockLong = pd.merge(dfCleanMetadata,dfFlockPivot, how=\"inner\", left_on=\"globalid_y\", right_on=\"globalid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder, clean, and rename columns\n",
    "dfFlockLong = (\n",
    "    pd.DataFrame(dfFlockLong)\n",
    "    .select_columns(['globalid_x', 'globalid_y', 'SiteName', 'ObserverText', 'EffortDate_Text', 'EffortTime', 'LocationDescription', 'LocationOther', 'shortName', 'scientificName', 'fwsTaxonCode', 'ITISTaxonCode', 'value','LatitudeDD', 'LongitudeDD', 'EffortNotes'])\n",
    "    .rename_columns(new_column_names={\"globalid_x\": \"eventID\", \"globalid_y\": \"occurrenceID\", \"SiteName\": \"location\", \"EffortTime\": \"eventTime\", \"EffortDate_Text\": \"eventDate\", \"EffortNotes\": \"eventRemarks\", \"LatitudeDD\": \"decimalLatitude\", \"LongitudeDD\": \"decimalLongitude\", \"ObserverText\": \"recordedBy\", \"value\": 'individualCount'})\n",
    ")\n",
    "dfFlockLong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create wide flock data, e.g., species counts in columns not rows\n",
    "dfFlockWide = (\n",
    "    pd.DataFrame(sedfMetadataYYYY_ObservationPoint)\n",
    "    .select_columns(['globalid_x', 'globalid_y', 'SiteName', 'ObserverText', 'EffortDate_Text', 'EffortTime', 'LocationDescription', 'LocationOther', 'DuskyCount', 'WuskyCount', 'WesternCount', 'TavLessCount', 'TavCount', 'LessCount', 'AleutianCount', 'CacklingCount', 'UnknownCanadaCount', 'UnknownCacklerCount', 'WhiteFrontedCount', 'SnowCount', 'RossCount', 'UnknownGooseCount', 'TrumpeterCount', 'TundraCount', 'UnknownSwanCount', 'Total', 'LatitudeDD', 'LongitudeDD', 'EffortNotes'])\n",
    "    .rename_columns(new_column_names={\"globalid_x\": \"eventID\", \"globalid_y\": \"occurrenceID\", \"SiteName\": \"location\", \"EffortTime\": \"eventTime\", \"EffortDate_Text\": \"eventDate\", \"EffortNotes\": \"eventRemarks\", \"LatitudeDD\": \"decimalLatitude\", \"LongitudeDD\": \"decimalLongitude\", \"ObserverText\": \"recordedBy\"})\n",
    ")\n",
    "dfFlockWide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export csvs. Uncomment to write location-specific flock and neckband csvs\n",
    "\n",
    "## Create separate csvs for each unique location for long flock data\n",
    "#for c in dfFlockLong.location.unique():\n",
    "#    (dfFlockLong[dfFlockLong.location == c]).to_csv(os.path.join(out_workspace,(c + '_Goose_Mark-Resight_FlockLong_' + timestamp + '.csv')), index= False)\n",
    "    \n",
    "## Create separate csvs for each unique location for wide flock data\n",
    "#for c in dfFlockWide.location.unique():\n",
    "#    (dfFlockWide[dfFlockWide.location == c]).to_csv(os.path.join(out_workspace,(c + '_Goose_Mark-Resight_FlockWide_' + timestamp + '.csv')), index= False)\n",
    "\n",
    "## Create separate csvs for each unique location for dusky neckband data\n",
    "#for c in dfDuskyNeckbandNWR.location.unique():\n",
    "#    (dfDuskyNeckbandNWR[dfDuskyNeckbandNWR.location == c]).to_csv(os.path.join(out_workspace,(c + '_Goose_Mark-Resight_Neckband_' + timestamp + '.csv')), index= False)\n",
    "\n",
    "## Create csv of dusky neckband data for Migratory Birds\n",
    "dfDuskyNeckbandMB.to_csv(os.path.join(out_workspace,('USFWS_Goose_Mark-Resight_Neckband_' + timestamp + '.csv')), index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export Excel spreadsheets\n",
    "\n",
    "## Create separate Excel spreadsheets for each unique location\n",
    "for c in sedfMetadataYYYY_ObservationPoint.SiteName.unique():\n",
    "    writer = pd.ExcelWriter(os.path.join(out_workspace,(c + '_Goose_Data_' + timestamp + '.xlsx')))\n",
    "    dfDuskyNeckbandNWR[dfDuskyNeckbandNWR.location == c]\n",
    "    dfFlockLong[dfFlockLong.location == c].to_excel(writer, 'Flock Long', index=False)\n",
    "    dfFlockWide[dfFlockWide.location == c].to_excel(writer, 'Flock Wide', index=False)\n",
    "    dfDuskyNeckbandNWR[dfDuskyNeckbandNWR.location == c].to_excel(writer, 'Dusky Neckband', index=False)\n",
    "    writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
